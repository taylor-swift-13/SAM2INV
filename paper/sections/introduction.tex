%% ============================================================
\section{Introduction}\label{sec:intro}
%% ============================================================

Loop invariants are the cornerstone of deductive program verification.
Given a loop annotated with an appropriate invariant, a verification condition generator
such as Frama-C/WP~\cite{framac} can reduce the correctness of the entire program to
a set of first-order proof obligations dischargeable automatically.
In practice, however, \emph{finding} the right invariant is the bottleneck:
even expert users spend substantial effort crafting invariants that are simultaneously
\emph{inductive} (preserved by each iteration), \emph{sufficient} (strong enough to
imply the postcondition), and \emph{syntactically admissible} (expressible in the
annotation language).

Classical approaches fall into two broad camps.
\emph{Static} techniques such as abstract interpretation~\cite{cousot1977abstract},
template-based constraint solving~\cite{colon2003linear}, and
Houdini-style iterative weakening~\cite{flanagan2001houdini}
offer soundness guarantees but are confined to fixed abstract domains or templates.
\emph{Dynamic} techniques infer candidates from execution traces using polynomial
fitting~\cite{daikon}, machine learning~\cite{si2018learning}, or data-driven
enumerative search~\cite{garg2016learning}, but must still close the gap between
empirical observation and formal proof.

Recently, large language models~(LLMs) have demonstrated remarkable ability at code
understanding and generation.
Several studies have explored LLM-driven invariant
synthesis~\cite{chakraborty2024towards,pei2023can,kamath2023finding},
showing that LLMs can produce plausible candidates even for non-trivial programs.
However, existing approaches suffer from two structural weaknesses.
First, they rely on a single LLM call or a simple repair loop without \emph{systematic
multi-stage filtering}, so many returned candidates are syntactically ill-formed,
semantically inconsistent with program behaviour, or formally non-inductive.
Second, the verification signal produced by these systems is discarded after each
run; no mechanism exists for using that signal to \emph{improve} the LLM policy
that generates the candidates.
The result is that each new program is treated as if the model had learned nothing
from prior successes and failures.

\paragraph{Why invariant quality is hard to learn from.}
A fundamental obstacle to improving LLM-based invariant generation
through reinforcement learning is that
\emph{evaluating candidate quality is structurally different here
from other code generation tasks}.
The correct annotation for a loop is not a single program but a
\emph{conjunction} of multiple invariant clauses, each capturing a
different arithmetic relationship.
This has two consequences.
First, the binary verifier signal is \emph{non-attributable}:
the final correct annotation is typically assembled from contributions
of several parallel LLM calls, so no individual call can be scored
as simply correct or incorrect.
Second, the binary signal is susceptible to \emph{reward hacking}:
trivially weak annotations (e.g., \lstinline|loop invariant true;|)
can formally pass a checker on programs with loose postconditions
yet carry no mathematical insight about the loop.
These two properties together explain why online reinforcement
learning methods such as
GRPO~\cite{deepseekr1}---which score individual completions against
each other via binary verifier feedback---are poorly suited to this
domain.

\paragraph{Our approach.}
We present \textsc{SAM2INV} (\textbf{Sam}pling \textbf{to}
\textbf{Inv}ariant), a system that addresses both problems through a
two-stage pipeline: first generate a large, diverse corpus of loop
programs; then for each program, generate, filter, and certify
invariant candidates---logging every accepted and rejected expression
as structured training data.

\textbf{Contribution~1: \textsc{LoopFactory}---a Probabilistic DSL for Task Input Generation.}
Existing benchmarks contain only a few dozen loop programs, which
is insufficient for training.
We design \textsc{LoopFactory}, a probabilistic domain-specific
language~(DSL) that generates structured numeric loop programs with
tunable arithmetic complexity (linear vs.\ nonlinear), nesting depth,
variable count, and loop-control mode.
\textsc{LoopFactory} is the \emph{task input generator}: it produces
the programs that the invariant-generation pipeline annotates, and
hence the programs that ultimately populate the SFT and DPO training
corpora.
Without a scalable supply of programs with known structure, offline
training data cannot be generated at the scale needed for fine-tuning.

\textbf{Contribution~2: A Three-Stage Sampling-and-Filtering Pipeline.}
Given a program (from \textsc{LoopFactory} or any source),
\textsc{SAM2INV} issues $N$ parallel LLM calls and \emph{unions} all
returned candidate invariant expressions into a single pool, rather
than selecting among them.
The pool is then filtered through three progressively more expensive
stages:
(i)~an ACSL \emph{syntax filter} that rejects ill-formed expressions
without running any code;
(ii)~an \emph{execution-trace semantic filter} that discards candidates
falsified by any observed loop state;
and (iii)~\emph{Houdini formal pruning} that uses Frama-C/WP to
remove non-inductive candidates, converging to the strongest
inductive conjunction the union supports.
Every survivor is \emph{certified} by Frama-C/WP.
Importantly, \emph{the filtering pipeline alone---before any
fine-tuning---already yields substantial improvements in both
success rate and invariant quality} over a single-call baseline.
Union assembly makes the final annotation stronger by combining
conjuncts that different calls discover independently;
the syntax and trace filters ensure that only well-formed,
semantically consistent candidates reach Frama-C, eliminating
parse failures and reducing the cost of Houdini pruning;
and Houdini finds the maximal inductive conjunction the
pre-screened union can support.
The pipeline is therefore a strong standalone tool for invariant
generation, and the offline training data collection is an
additional improvement layer built on top of it.
Running \textsc{LoopFactory} and this pipeline together at scale
constitutes the training data generation workflow: each program
yields one certified annotation and a set of stage-labelled rejected
candidates.

\textbf{Contribution~3: Offline SFT and DPO Data from Pipeline Logs.}
The union-then-filter design makes online RL inapplicable for the
reasons above, but it naturally produces structured training data
\emph{at no extra cost}: the pipeline already logs every candidate
and the stage at which it was filtered.
\textsc{SAM2INV} turns these logs into SFT records (program paired
with its certified annotation) and DPO preference pairs (certified
annotation as chosen; each rejected candidate as rejected, labelled
by its rejection stage).
This stage-stratified labelling provides a richer signal than binary
pass/fail, avoids reward hacking, and requires no human annotation
or online reward model.

\medskip
The remainder of this paper is organised as follows.
Section~\ref{sec:related} surveys related work.
Section~\ref{sec:overview} gives a high-level overview of the \textsc{SAM2INV} pipeline.
Section~\ref{sec:method} details each component.
Section~\ref{sec:loop-factory} describes \textsc{LoopFactory}.
Section~\ref{sec:experiments} presents experimental evaluation.
Section~\ref{sec:conclusion} concludes.
