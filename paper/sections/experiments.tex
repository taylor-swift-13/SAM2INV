%% ============================================================
\section{Experiments}\label{sec:experiments}
%% ============================================================

%% TODO: Fill in experimental results.
%% Suggested structure:
%%
%% \subsection{Experimental Setup}
%%   - Benchmarks: NLA suite, LoopFactory-generated programs
%%   - LLM models: GPT-4o, GPT-5-nano, DeepSeek-V3.1
%%   - Baselines: Daikon, DIG, direct GPT-4 prompting
%%   - Metrics: success rate, #LLM queries, #Frama-C calls, token cost
%%   - Hardware / timeout configuration
%%
%% \subsection{RQ1: Overall Effectiveness}
%%   - Table: per-benchmark success rate
%%   - Comparison with baselines
%%
%% \subsection{RQ2: Ablation Study}
%%   - Effect of smart sampling vs. random sampling
%%   - Effect of parallel diverse generation (N=1 vs N=5 vs N=20)
%%   - Effect of Houdini pruning
%%   - Effect of generation mode (code_only vs fit_only vs hybrid)
%%
%% \subsection{RQ3: Scalability on Synthetic Benchmarks}
%%   - Success rate vs. program complexity (nesting depth, variable count)
%%   - Token cost analysis
%%
%% \subsection{RQ4: Vector Cache Effectiveness}
%%   - Cache hit rate over time
%%   - Reduction in LLM queries for similar programs
%%
%% \subsection{Threats to Validity}
