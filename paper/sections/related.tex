%% ============================================================
\section{Related Work}\label{sec:related}
%% ============================================================

\paragraph{Classical Invariant Synthesis.}
Abstract interpretation~\cite{cousot1977abstract} computes fixed-point
approximations over abstract domains (intervals, octagons, polyhedra)
and has been widely adopted in industrial static analysers.
Template-based approaches~\cite{colon2003linear,sankaranarayanan2004non}
reduce invariant synthesis to constraint solving by fixing a parametric
template and searching for satisfying coefficients.
The Houdini algorithm~\cite{flanagan2001houdini} takes the dual approach:
it starts with a large candidate set and iteratively removes conjuncts
that are not inductive, converging to the strongest conjunction that is
preserved by the loop.
While sound, these methods are inherently limited by the expressiveness
of their abstract domains or template families.

\paragraph{Dynamic Invariant Detection.}
Daikon~\cite{daikon} pioneered the idea of mining likely invariants
from execution traces by instantiating templates over observed variable
values.
Subsequent work extended this idea with more expressive templates,
statistical hypothesis testing, and machine-learning
classifiers~\cite{garg2016learning}.
DIG~\cite{nguyen2012using} combines dynamic analysis with symbolic
execution to discover non-linear polynomial invariants.
These approaches are effective at generating plausible candidates
but provide no formal guarantee; a separate verification step is
required to confirm inductiveness.

\paragraph{LLM-Based Invariant Generation.}
The emergence of large language models has opened a new avenue for
invariant synthesis.
Chakraborty and Lahiri~\cite{chakraborty2024towards} demonstrate that
GPT-4 can generate loop invariants for simple programs when given
appropriate prompts and a verification feedback loop.
Pei et al.~\cite{pei2023can} study the ability of code-trained LLMs
to produce Hoare-style annotations, finding that models can often
produce correct invariants but benefit significantly from iterative
refinement.
Kamath et al.~\cite{kamath2023finding} propose a ``guess-and-check''
framework that combines LLM generation with bounded model checking.
Compared to these works, \textsc{SAM2INV} introduces three
distinguishing features:
(i)~systematic use of execution traces from smart dynamic sampling
to ground the LLM's generation;
(ii)~parallel diverse generation with multiple prompts and temperatures;
and (iii)~a Houdini-pruning integration that provides monotonic
progress guarantees within the repair loop.

\paragraph{Program Synthesis with LLMs.}
More broadly, LLMs have been applied to a variety of synthesis tasks
including code generation~\cite{chen2021evaluating},
test generation~\cite{lemieux2023codamosa},
and specification inference~\cite{endres2024can}.
Our work shares the spirit of using LLMs as proposal generators
within a verify-and-refine loop, but targets the specific domain
of ACSL loop invariants and integrates domain-specific filtering
(trace validation, ACSL syntax checking) to dramatically reduce
the verification burden.

\paragraph{Benchmark Generation for Verification.}
Existing invariant synthesis benchmarks---such as the NLA
(Non-Linear Arithmetic) suite from SV-COMP~\cite{svcomp}---contain
only a few dozen programs.
To enable larger-scale evaluation, we introduce a probabilistic
DSL for synthesizing numeric loop programs, drawing inspiration
from grammar-based fuzzing~\cite{godefroid2008grammar} and
probabilistic program synthesis~\cite{nori2015efficient}.
